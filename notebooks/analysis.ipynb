{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "#add path\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(\"../telegram_messages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts.CoNLL_formatting'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCoNLL_formatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scripts.CoNLL_formatting'"
     ]
    }
   ],
   "source": [
    "from scripts.CoNLL_formatting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         channel         sender                  timestamp  \\\n",
      "0  @ZemenExpress -1001307493052  2025-03-08 13:58:44+00:00   \n",
      "1  @ZemenExpress -1001307493052  2025-03-08 11:35:13+00:00   \n",
      "2  @ZemenExpress -1001307493052  2025-03-08 11:34:52+00:00   \n",
      "3  @ZemenExpress -1001307493052  2025-03-07 13:33:41+00:00   \n",
      "4  @ZemenExpress -1001307493052  2025-03-07 13:33:40+00:00   \n",
      "\n",
      "                                             content  \\\n",
      "0  ðŸ’¥ðŸ’¥...............ðŸŒž.................ðŸ’¥ðŸ’¥\\n\\nâ“ á‰ áˆ¨á...   \n",
      "1  ðŸ’¥ðŸ’¥....................................ðŸ’¥ðŸ’¥\\n\\nðŸ“ŒL...   \n",
      "2  ðŸ’¥ðŸ’¥....................................ðŸ’¥ðŸ’¥\\n\\nðŸ“ŒL...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                              tokens  \n",
      "0  ['ðŸ’¥ðŸ’¥...............ðŸŒž.................ðŸ’¥ðŸ’¥', 'â“',...  \n",
      "1  ['ðŸ’¥ðŸ’¥....................................ðŸ’¥ðŸ’¥', '...  \n",
      "2  ['ðŸ’¥ðŸ’¥....................................ðŸ’¥ðŸ’¥', '...  \n",
      "3                                                 []  \n",
      "4                                                 []  \n",
      "Index(['channel', 'sender', 'timestamp', 'content', 'tokens'], dtype='object')\n",
      "Labeled data saved to labeled_data.conll\n"
     ]
    }
   ],
   "source": [
    "# Read the data and save the subset_df in a variable\n",
    "data = 'preprocessed_data.csv'\n",
    "subset_df = read_subset_df(data)\n",
    "\n",
    "# Call the label_data function to process and save the labeled data\n",
    "label_data(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts.load_conll_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload_conll_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_alignment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_and_tuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scripts.load_conll_dataset'"
     ]
    }
   ],
   "source": [
    "from scripts.load_conll_dataset import *\n",
    "from scripts.tokenization_alignment import *\n",
    "from scripts.training_and_tuning import *\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables for api\n",
    "token = os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_conll_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the connl data that we made earlier\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33mlabeled_data.conll\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sentences, labels = \u001b[43mload_conll_dataset\u001b[49m(file_path)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_conll_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the connl data that we made earlier\n",
    "file_path = 'labeled_data.conll'\n",
    "sentences, labels = load_conll_dataset(file_path)\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Optionally, print a sample to verify\n",
    "for i in range(min(5, len(sentences))):  # Print up to 5 samples\n",
    "    print(f\"Sentence {i+1}: {sentences[i]}\")\n",
    "    print(f\"Labels {i+1}: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_codes()\n",
    "# sentences, labels = load_conll_dataset('labeled_data.conll')\n",
    "\n",
    "# Define label mapping before loading the model\n",
    "label_set = set(label for label_list in labels for label in label_list)\n",
    "label_map = {label: i for i, label in enumerate(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "# rasyosef/bert-tiny-amharic\n",
    "model_name = \"rasyosef/bert-tiny-amharic\"  # Change to \"bert-tiny-amharic\" or \"afroxmlr\" as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming sentences and labels are already loaded\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Ensure they are not empty before proceeding\n",
    "if len(sentences) == 0 or len(labels) == 0:\n",
    "    raise ValueError(\"Sentences or labels are empty. Please check your data loading process.\")\n",
    "\n",
    "# Use all data for training if there's only one sentence\n",
    "if len(sentences) < 2:\n",
    "    print(\"Not enough data to split into training and validation sets. Using all data for training.\")\n",
    "    train_sentences = sentences\n",
    "    train_labels = labels\n",
    "    val_sentences = []\n",
    "    val_labels = []\n",
    "else:\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "        sentences, labels, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "# Tokenize and align labels for train and validation sets\n",
    "train_tokenized = tokenize_and_align_labels(train_sentences, train_labels, tokenizer, label_map)\n",
    "\n",
    "# If there are validation sets, tokenize those as well\n",
    "if val_sentences:\n",
    "    val_tokenized = tokenize_and_align_labels(val_sentences, val_labels, tokenizer, label_map)\n",
    "else:\n",
    "    val_tokenized = None\n",
    "\n",
    "# Create a dictionary for the tokenized dataset\n",
    "tokenized_dataset = {\n",
    "    'train': train_tokenized,\n",
    "    'validation': val_tokenized\n",
    "}\n",
    "\n",
    "print(\"Tokenized training dataset:\")\n",
    "print(tokenized_dataset['train'])\n",
    "print(\"Keys in training dataset:\", tokenized_dataset['train'].keys())\n",
    "\n",
    "num_examples = len(tokenized_dataset['train']['input_ids'])\n",
    "print(f\"Number of examples in the training dataset: {num_examples}\")\n",
    "\n",
    "for key in tokenized_dataset['train'].keys():\n",
    "    print(f\"Length of {key}: {len(tokenized_dataset['train'][key])}\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    if (len(tokenized_dataset['train']['input_ids'][i]) == 0 or\n",
    "        len(tokenized_dataset['train']['labels'][i]) == 0):\n",
    "        print(f\"Empty entry found in index {i}: input_ids length: {len(tokenized_dataset['train']['input_ids'][i])}, \"\n",
    "              f\"labels length: {len(tokenized_dataset['train']['labels'][i])}\")\n",
    "# Ensure the labels are not empty\n",
    "if 'labels' not in tokenized_dataset['train'] or not tokenized_dataset['train']['labels']:\n",
    "    raise ValueError(\"Labels cannot be empty.\")\n",
    "\n",
    "# Check for length mismatches in the tokenized dataset\n",
    "for i in range(len(tokenized_dataset['train']['input_ids'])):\n",
    "    if (len(tokenized_dataset['train']['input_ids'][i]) !=\n",
    "        len(tokenized_dataset['train']['token_type_ids'][i]) or\n",
    "        len(tokenized_dataset['train']['attention_mask'][i]) !=\n",
    "        len(tokenized_dataset['train']['labels'][i])):\n",
    "        print(f\"Length mismatch at index {i}: \"\n",
    "              f\"{len(tokenized_dataset['train']['input_ids'][i])}, \"\n",
    "              f\"{len(tokenized_dataset['train']['token_type_ids'][i])}, \"\n",
    "              f\"{len(tokenized_dataset['train']['attention_mask'][i])}, \"\n",
    "              f\"{len(tokenized_dataset['train']['labels'][i])}\")\n",
    "\n",
    "# Ensure lengths are consistent\n",
    "for i in range(len(tokenized_dataset['train']['input_ids'])):\n",
    "    input_len = len(tokenized_dataset['train']['input_ids'][i])\n",
    "    token_type_len = len(tokenized_dataset['train']['token_type_ids'][i])\n",
    "    attention_mask_len = len(tokenized_dataset['train']['attention_mask'][i])\n",
    "    labels_len = len(tokenized_dataset['train']['labels'][i])\n",
    "    \n",
    "    if not (input_len == token_type_len == attention_mask_len == labels_len):\n",
    "        print(f\"Length mismatch at index {i}: input_ids({input_len}), \"\n",
    "              f\"token_type_ids({token_type_len}), \"\n",
    "              f\"attention_mask({attention_mask_len}), \"\n",
    "              f\"labels({labels_len})\")\n",
    "    else:\n",
    "        print(f\"Index {i} is consistent: input_ids({input_len}), \"\n",
    "              f\"token_type_ids({token_type_len}), \"\n",
    "              f\"attention_mask({attention_mask_len}), \"\n",
    "              f\"labels({labels_len})\")\n",
    "        \n",
    "# Proceed with fine-tuning if everything is correct\n",
    "trainer = fine_tune_model(tokenized_dataset['train'], val_tokenized, model, tokenizer)\n",
    "\n",
    "# If you have a validation set, you can still evaluate it later\n",
    "if val_tokenized:\n",
    "    eval_results = evaluate_model(trainer)\n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels\n",
    "tokenized_dataset = tokenize_and_align_labels(sentences, labels, tokenizer, label_map)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer = fine_tune_model(tokenized_dataset['train'], tokenized_dataset['validation'], model, tokenizer)\n",
    "\n",
    "# # Evaluate the model\n",
    "# eval_results = evaluate_model(trainer)\n",
    "# print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "save_model(trainer, \"fine_tuned_ner_model\", tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
